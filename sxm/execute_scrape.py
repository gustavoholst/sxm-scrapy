from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from settings import FEED_URI

import os
import ast
import pandas


def run_spider():
	"""Run spider for scraping sxm website

	Results of scrape stored to json file (see settings.py)
	"""
	process = CrawlerProcess(get_project_settings())
	process.crawl('sxm')
	process.start()	


def get_now_playing():
	"""Read json file generated by spider and add lines to channel logs"""
	with open(FEED_URI[7:],'r') as f:
		now_playing = ast.literal_eval(f.read())
	print('NOW PLAYING:')
	for song in now_playing:
		print(song['channel']+':',song['artist'],'-',song['title'])
	return now_playing


def clear_json():
	"""Delete json file from previous crawl

	spider needs an empty json file to write to
	"""
	try:
		os.remove(FEED_URI[7:])
	except OSError:
		pass

def update_log(song):
	log_path = os.path.dirname(os.path.realpath(__file__)) + '\\channel_logs\\' + song['channel'] + '_log.log'
	if os.path.isfile(log_path):
        log = pd.read_csv(log_path,index_col=0)
        print(song['channel'], 'table loaded from', path)
        nrows = len(log.index)
        prev_songs = []
        for i in range(-3,0):
        	if log.iloc(nrows+i)['title'] == song['title']:
        		match = 1
        		break
    		else:
    			match = 0
		if match == 1:
			print('Song already logged!')
		else:
			log = log.append(song,ignore_index = True)
			log.to_csv(log_path)

    else:
        log = pd.DataFrame(data=None, columns = ['artist', 'title', 'albumart', 'time'], index = None)
        log.to_csv(log_path)
        print('Blank', channel, 'table created @', path)


def main():
	"""Execuse scraping activities

	Crawl using srapy spider, read results, write to channel logs
	"""
	clear_json()
	run_spider()
	now_playing = get_now_playing()
	for song in now_playing:
		update_log(song)

if __name__ == '__main__':
    main()

